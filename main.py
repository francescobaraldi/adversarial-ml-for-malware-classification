import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
from torch import nn
from torch.utils.data import DataLoader
import gurobipy as gp
from gurobipy import GRB
import os
from tqdm import tqdm

from MILP import compute_adv_instance, compute_bounds, read_bounds
from nets import Net, eval_acc
from dataset import MyDataset


ROOT_DIR = os.getcwd()
SEED = 21
torch.manual_seed(SEED)

# NB: set to False in order to load the weights from file and not train the model from scratch
TRAIN_FROM_SCRATCH = False

# Load the dataset
df = pd.read_csv("dataset_malware/drebin-215-dataset-5560malware-9476-benign.csv")
x = df.drop(columns=['class']).to_numpy().astype(np.float32)
y = df['class'].to_numpy()
# 'B' = bening -> '1'
# 'S' = malware -> '0'
y = np.where(y == 'B', 1.0, 0.0).astype(np.float32)

BATCH_SIZE = 128
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=SEED, shuffle=True)
train_dataset = MyDataset(x_train, y_train)
test_dataset = MyDataset(x_test, y_test)

dl_train = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)
dl_test = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False)

# Reti da testare
#
# [8, 8, 8]
# [20, 10, 8, 8]
# [20, 20, 10, 10, 10]
# [50, 50]
# [100, 100]
#

num_epochs = 50
dim_input = 215
dim_output = 1
hidden_layers = [8, 8, 8]
learning_rate = 0.01
model = Net(dim_input, hidden_layers, dim_output)

# Prepare the DNN (train from scratch or load the weights from file)
if TRAIN_FROM_SCRATCH:
  loss_fun = nn.BCELoss()
  opt = torch.optim.SGD(model.parameters(), learning_rate)
  
  train_acc = 0
  test_acc = 0
  for i in range(num_epochs):
    train_acc = eval_acc(model, dl_train)
    test_acc = eval_acc(model, dl_test)

    for x, y in dl_train:
      opt.zero_grad()
      y = y.unsqueeze(1).to(torch.float32)
      y_pred = model(x)
      loss = loss_fun(y_pred, y)
      loss.backward()
      opt.step()
    
    print(f"Epoch {i} train acc.: {train_acc:.3f} - test acc.: {test_acc:.3f}")
  
  # Save the weights to file
  torch.save(model.state_dict(), ROOT_DIR + f"/params/net_{hidden_layers}_params.pth")
else:
  # Load the weights from file
  model.load_state_dict(torch.load(ROOT_DIR + f"/params/net_{hidden_layers}_params.pth"))

# Prepare the lists for weights and biases
layers = list(model.children())[0]
weights = []
biases = []
for layer in layers:
    if type(layer) == type(nn.Linear(1, 1)):
        weights.append(layer.state_dict()['weight'])
        biases.append(layer.state_dict()['bias'])

# Collect all the samples in the testset classified as "malware"
x_tests = []  # List that contains all the sample classified as malware
for x, y in dl_test:
    for i, el in enumerate(y):
        if el == 0.0:
            x_tests.append(x[i])

# Prepare the optimized bounds (compute them from scratch or load from file)
x_test = x_tests[0]
x_bounds = []
s_bounds = []
if TRAIN_FROM_SCRATCH:
    # Compute the improved bounds for each neuron of each layer
    for k in range(len(weights) - 1):
        x_k = []
        s_k = []
        for j in range(weights[k].shape[0]):
            x, s = compute_bounds(weights, biases, x_test, k, j, MAX_ERROR=20, TIME_LIMIT=10, LOG=False)
            x_k.append(x)
            s_k.append(s)
        x_bounds.append(x_k)
        s_bounds.append(s_k)
    
    # Save them to file
    bounds_file = open(ROOT_DIR + f"/bounds/net_{hidden_layers}_bounds.csv", "w")
    for k in range(len(weights) - 1):
        bounds_file.write(f"x_{k},s_{k},")
    max_depth = max(hidden_layers)
    for j in range(max_depth):
        bounds_file.write("\n")
        for k in range(len(weights) - 1):
            if hidden_layers[k] <= j:
                bounds_file.write(",,")
            else:
                bounds_file.write(f"{x_bounds[k][j]},{s_bounds[k][j]},")
    bounds_file.close()
else:
    # Load the improved bounds from file
    x_bounds, s_bounds = read_bounds(ROOT_DIR + f"/bounds/net_{hidden_layers}_bounds.csv", hidden_layers)

bounds = {
    "x_bounds": x_bounds,
    "s_bounds": s_bounds
}

# Testing the MILP approach
TIME_LIMIT = 2 * 60
filename = f"/outputs/net_{hidden_layers}.txt"
output_file = open(ROOT_DIR + filename, "w")
output_file.write(f"Rete: {hidden_layers}")
num_opt = 0
num_solved_timelimit = 0
num_unsolved = 0
num_test_instances = 50
times = []
gaps = []
for x_test in tqdm(x_tests[:num_test_instances + 1]):
    adversarial_example, runtime, gap = compute_adv_instance(weights=weights, biases=biases, input=x_test, MAX_ERROR=20, TIME_LIMIT=TIME_LIMIT, LOG=False)
    if adversarial_example is None:
        num_unsolved += 1
        output_file.write("\n--------------------------------------------------------------------------")
        output_file.write("\nThe model didn't compute any feasible solution.")
        output_file.write("\n--------------------------------------------------------------------------")
    else:
        times.append(runtime)
        gaps.append(gap)
        adversarial_example[[adversarial_example == -0.0]] = 0.0
        output_file.write("\n--------------------------------------------------------------------------")
        if gap == 0:
            num_opt += 1
        else:
            num_solved_timelimit += 1
        output_file.write(f"\nSolution found in {runtime:.3f} s")
        output_file.write(f"\nGap: {gap * 100} %")
        output_file.write(f"\nNumbers of features changed: {torch.sum(adversarial_example != x_test).item()}")
        output_file.write(f"\nThe original classification is {model(torch.unsqueeze(x_test, 0)).item():.3f}")
        output_file.write(f"\nThe classification of the adversarial example is {model(torch.unsqueeze(adversarial_example, 0)).item():.3f}")
        output_file.write("\n--------------------------------------------------------------------------")
output_file.write(f"\n\nNumber of instances unsolved: {num_unsolved} ({(num_unsolved / num_test_instances) * 100:.1f} %)")
output_file.write(f"\nNumber of instances solved within time limit: {num_opt} ({(num_opt / num_test_instances) * 100:.1f} %)")
output_file.write(f"\nNumber of instances solved exceeding the time limit: {num_solved_timelimit} ({(num_solved_timelimit / num_test_instances) * 100:.1f} %)")
if len(times) != 0 and len(gaps) != 0:
    avg_time = sum(times) / len(times)
    avg_gap = sum(gaps) / len(gaps)
    output_file.write(f"\nAverage time: {avg_time:.3f} s")
    output_file.write(f"\nAverage gap: {avg_gap * 100:.3f} %")
else:
    output_file.write(f"\nAll instances unsolved.")
output_file.close()

# Testing the improved MILP approach
TIME_LIMIT = 2 * 60
filename = f"/outputs/net_{hidden_layers}_improved.txt"
output_file = open(ROOT_DIR + filename, "w")
output_file.write(f"Rete: {hidden_layers}")
num_opt = 0
num_solved_timelimit = 0
num_unsolved = 0
num_test_instances = 50
times = []
gaps = []
for x_test in tqdm(x_tests[:num_test_instances + 1]):
    adversarial_example, runtime, gap = compute_adv_instance(weights=weights, biases=biases, input=x_test, bounds=bounds, MAX_ERROR=20, TIME_LIMIT=TIME_LIMIT, LOG=False)
    if adversarial_example is None:
        num_unsolved += 1
        output_file.write("\n--------------------------------------------------------------------------")
        output_file.write("\nThe model didn't compute any feasible solution.")
        output_file.write("\n--------------------------------------------------------------------------")
    else:
        times.append(runtime)
        gaps.append(gap)
        adversarial_example[[adversarial_example == -0.0]] = 0.0
        output_file.write("\n--------------------------------------------------------------------------")
        if gap == 0:
            num_opt += 1
        else:
            num_solved_timelimit += 1
        output_file.write(f"\nSolution found in {runtime:.3f} s")
        output_file.write(f"\nGap: {gap * 100} %")
        output_file.write(f"\nNumbers of features changed: {torch.sum(adversarial_example != x_test).item()}")
        output_file.write(f"\nThe original classification is {model(torch.unsqueeze(x_test, 0)).item():.3f}")
        output_file.write(f"\nThe classification of the adversarial example is {model(torch.unsqueeze(adversarial_example, 0)).item():.3f}")
        output_file.write("\n--------------------------------------------------------------------------")
output_file.write(f"\n\nNumber of instances unsolved: {num_unsolved} ({(num_unsolved / num_test_instances) * 100:.1f} %)")
output_file.write(f"\nNumber of instances solved within time limit: {num_opt} ({(num_opt / num_test_instances) * 100:.1f} %)")
output_file.write(f"\nNumber of instances solved exceeding the time limit: {num_solved_timelimit} ({(num_solved_timelimit / num_test_instances) * 100:.1f} %)")
if len(times) != 0 and len(gaps) != 0:
    avg_time = sum(times) / len(times)
    avg_gap = sum(gaps) / len(gaps)
    output_file.write(f"\nAverage time: {avg_time:.3f} s")
    output_file.write(f"\nAverage gap: {avg_gap * 100:.3f} %")
else:
    output_file.write(f"\nAll instances unsolved.")
output_file.close()
